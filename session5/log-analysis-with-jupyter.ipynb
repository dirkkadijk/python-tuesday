{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Use Case: Log analysis with Jupyter Notebook\n",
    "\n",
    "## Python Tuesday: Session 5\n",
    "\n",
    "<!-- \n",
    "\n",
    ":date: 2019-11-22\n",
    ":author: GÃ¡bor Nyers\n",
    ":tags: python\n",
    ":category: python workshop\n",
    ":summary: Log analysis with Jupyter Notebook\n",
    ":lang: en\n",
    ":licence: CC BY-NC 4.0 <https://creativecommons.org/licenses/by-nc/4.0/>\n",
    "\n",
    "-->\n",
    "\n",
    "## Preliminary considerations:\n",
    "\n",
    "- **Goal:** This notebook is meant to illustrate how a Jupyter notebook can be helpful for ad-hoc log analysis.\n",
    "- **Out of th box Python, as much as possible:** There are numerous Python modules that can be useful for analysis (e.g.: ``pandas``), this notebook prefers to use only the Python Standard library. Notable exception is the Jupyter Notebook module(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in file\n",
    "\n",
    "When processing log files we need to keep a few things in mind:\n",
    "\n",
    "- **performance**: Reading from disk is orders of magnitude slower than reading from RAM. To improve performance the data should be loaded into RAM.\n",
    "- **available RAM**: Log files may can be large so loading in them RAM may not be desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make an informed choice let's check the log file's size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of \"apache_logs-public-example\" is 2370789 bytes (2.26MB).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "logfile = 'apache_logs-public-example'\n",
    "logsize = os.path.getsize(logfile)\n",
    "print('The size of \"{}\" is {} bytes ({:.2f}MB).'.format(os.path.basename(logfile), logsize, logsize/1024**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to limit the amount of data being loaded into RAM we may set a limit. In the next cell the ``logmaxsize`` variable can be used to limit the amount of data to be processed.\n",
    "\n",
    "Read in the content of the log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 lines from logfile.\n"
     ]
    }
   ],
   "source": [
    "logmaxsize = logsize\n",
    "fh = open(logfile, 'r')\n",
    "content = fh.readlines(logmaxsize)\n",
    "fh.close()\n",
    "logrecords = len(content)\n",
    "print('Loaded {} lines from logfile.'.format(logrecords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample records\n",
    "\n",
    "Let's create a few helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_sample(collection, nr_of_samples=3):\n",
    "    '''Return ``nr_of_samples`` random sample from ``collection``\n",
    "    Return value: generator\n",
    "    '''\n",
    "    for i in range(nr_of_samples):\n",
    "        yield random.choice(collection)\n",
    "\n",
    "def head(collection, nr_of_elems=10):\n",
    "    '''Return a generator with ``nr_of_elems`` elements form ``collection``\n",
    "    '''    \n",
    "    for i, elem in enumerate(collection):\n",
    "        if i < nr_of_elems: yield elem\n",
    "        else: break\n",
    "            \n",
    "def getcol(collection, idx):\n",
    "    '''Assuming that ``collection`` is a table-like data structure, \n",
    "    return the ``idx``-th coloumn.\n",
    "    '''\n",
    "    for elem in collection:\n",
    "        if idx < len(elem): yield elem[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now examine 3 random samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198.46.149.143 - - [18/May/2015:11:05:00 +0000] \"GET /blog/geekery/disabling-battery-in-ubuntu-vms.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+semicomplete%2Fmain+%28semicomplete.com+-+Jordan+Sissel%29 HTTP/1.1\" 200 9316 \"-\" \"Tiny Tiny RSS/1.11 (http://tt-rss.org/)\"\n",
      " ------------------------------------------------------------\n",
      "65.55.213.73 - - [17/May/2015:14:05:34 +0000] \"GET /presentations/semantic-blogging HTTP/1.1\" 301 346 \"-\" \"msnbot/2.0b (+http://search.msn.com/msnbot.htm)\"\n",
      " ------------------------------------------------------------\n",
      "115.112.233.75 - - [19/May/2015:01:05:46 +0000] \"GET /favicon.ico HTTP/1.0\" 200 3638 \"-\" \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.107 Safari/537.36\"\n",
      " ------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sep = '-' * 60\n",
    "for elem in random_sample(content, 3):\n",
    "    print(elem, sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Format Analysis\n",
    "\n",
    "To process the log we'll need to figure out the different fields. In our example we intent to process an Apache access log, which, depening on the Apache configuration may contain different fields and possibly in custom order.\n",
    "\n",
    "### Field separation\n",
    "\n",
    "At first approximation the log record seem to be of CSV format, with <space> as separator and free text fields quoted. There is however a difference in the timestamp field: this is not quoted and has an extra space character.\n",
    "\n",
    "We have at least the following options to deal with the format:\n",
    "\n",
    "- Using the CSV module we can solve most of the problem of parsing, with some additional clean-up for the timestamp field.\n",
    "- Using the Python ``re`` module (Regular Expressions) we engineer an RE, which immediately parses the log records. The benefit of this approach is that - since it is a generic solution - it will probably be more maintainable for the long run. The relative complexity that enginering a suitable RegEx may be considered a drawback.\n",
    "\n",
    "\n",
    "### Parse the log\n",
    "\n",
    "Using regular expressions is a more generic solution, so let's try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 75.97.9.59\n",
      "2: -\n",
      "3: -\n",
      "4: 19/May/2015:01:05:36 +0000\n",
      "5: GET /presentations/logstash-puppetconf-2013/images/kibana3-1.png HTTP/1.1\n",
      "6: 200\n",
      "7: 425677\n",
      "8: http://semicomplete.com/presentations/logstash-puppetconf-2013/\n",
      "9: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.107 Safari/537.36\n",
      "------------------------------------------------------------\n",
      "1: 89.250.16.2\n",
      "2: -\n",
      "3: -\n",
      "4: 19/May/2015:10:05:43 +0000\n",
      "5: GET /favicon.ico HTTP/1.1\n",
      "6: 200\n",
      "7: 3638\n",
      "8: -\n",
      "9: Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.79 Safari/535.11\n",
      "------------------------------------------------------------\n",
      "1: 100.43.83.137\n",
      "2: -\n",
      "3: -\n",
      "4: 18/May/2015:10:05:56 +0000\n",
      "5: GET /blog/tags/scripting HTTP/1.1\n",
      "6: 200\n",
      "7: 8775\n",
      "8: -\n",
      "9: Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "apache_re = r'([\\d\\.]+) (.*?) (.*?) \\[(.*?)\\] \"(.* HTTP.*)\" (\\d+) (\\d+) \"(.*)\" \"(.*)\"'\n",
    "\n",
    "for rec in random_sample(content, 3):\n",
    "    try:\n",
    "        rec_parsed = re.match(apache_re, rec).groups()\n",
    "        for fieldnr, fieldval in enumerate(rec_parsed, 1):\n",
    "            print('{}: {}'.format(fieldnr, fieldval))\n",
    "        print('-' * 60)\n",
    "    except:\n",
    "        print('Could not parse log record:\\n', rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the entire log content using the above method and transform the log records to a Python data structure, e.g.: list of tuples.\n",
    "\n",
    "At this point we're fairly confident that our RegEx is correct for multiple records, but we have no certainty until we try to process the entire log content. For this reason we want to catch all records for whic the RegEx is not correct into a separate list: ``unprocessed_records``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unprocessed records: 670\n"
     ]
    }
   ],
   "source": [
    "content_parsed = []             # the correctly parsed records\n",
    "unprocessed_records = []        # the records not matching the RegEx\n",
    "for rec_id, rec in enumerate(content):\n",
    "    try:\n",
    "        parsed_rec = re.match(apache_re, rec).groups()\n",
    "        content_parsed.append(parsed_rec)\n",
    "    except Exception:\n",
    "        # Error handling: the current records did not match the RegEx, let's store it as uprocessed\n",
    "        unprocessed_records.append((rec_id, rec))\n",
    "print('Number of unprocessed records: {}'.format( len(unprocessed_records)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the number of unprocessed records is not 0, the our RegEx in ``apache_re`` does not match each record. This means that:\n",
    "\n",
    "1. either is our RegEx in ``apache_re`` incorrect, or\n",
    "2. some of the records are of a different format.\n",
    "\n",
    "It's prudent to assume 1. first, so let's try to examine a few records from the ``unprocecessed_records`` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76, '218.30.103.62 - - [17/May/2015:11:05:11 +0000] \"GET /robots.txt HTTP/1.1\" 200 - \"-\" \"Sogou web spider/4.0(+http://www.sogou.com/docs/help/webmasters.htm#07)\"\\n')\n",
      "(77, '218.30.103.62 - - [17/May/2015:11:05:46 +0000] \"GET /robots.txt HTTP/1.1\" 200 - \"-\" \"Sogou web spider/4.0(+http://www.sogou.com/docs/help/webmasters.htm#07)\"\\n')\n",
      "(85, '218.30.103.62 - - [17/May/2015:11:05:17 +0000] \"GET /projects/xdotool/xdotool.xhtml HTTP/1.1\" 304 - \"-\" \"Sogou web spider/4.0(+http://www.sogou.com/docs/help/webmasters.htm#07)\"\\n')\n",
      "RegEx: ([\\d\\.]+) (.*?) (.*?) \\[(.*?)\\] \"(.* HTTP.*)\" (\\d+) (\\d+) \"(.*)\" \"(.*)\"\n"
     ]
    }
   ],
   "source": [
    "for rec in head(unprocessed_records, 3):    \n",
    "    print(rec)\n",
    "print('RegEx:', apache_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem seems to be field 7. The current RegEx assumes that this field can only be a digit, but the examples of ``unprocessed_records`` list show that this field can also be a ``-`` (dash) character. Let's modify our RegEx accordingly, i.e.: ``(\\d+|-)`` instead of ``(\\d+)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 218.30.103.62\n",
      "2: -\n",
      "3: -\n",
      "4: 17/May/2015:11:05:11 +0000\n",
      "5: GET /robots.txt HTTP/1.1\n",
      "6: 200\n",
      "7: -\n",
      "8: -\n",
      "9: Sogou web spider/4.0(+http://www.sogou.com/docs/help/webmasters.htm#07)\n",
      "------------------------------------------------------------\n",
      "1: 218.30.103.62\n",
      "2: -\n",
      "3: -\n",
      "4: 17/May/2015:11:05:46 +0000\n",
      "5: GET /robots.txt HTTP/1.1\n",
      "6: 200\n",
      "7: -\n",
      "8: -\n",
      "9: Sogou web spider/4.0(+http://www.sogou.com/docs/help/webmasters.htm#07)\n",
      "------------------------------------------------------------\n",
      "1: 218.30.103.62\n",
      "2: -\n",
      "3: -\n",
      "4: 17/May/2015:11:05:17 +0000\n",
      "5: GET /projects/xdotool/xdotool.xhtml HTTP/1.1\n",
      "6: 304\n",
      "7: -\n",
      "8: -\n",
      "9: Sogou web spider/4.0(+http://www.sogou.com/docs/help/webmasters.htm#07)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "apache_re_modified = r'([\\d\\.]+) (.*?) (.*?) \\[(.*?)\\] \"(.* HTTP.*)\" (\\d+) (\\d+|-) \"(.*)\" \"(.*)\"'\n",
    "\n",
    "for rec_id, rec in head(unprocessed_records, 3):               # use the unprocessed_records !    \n",
    "    rec_parsed = re.match(apache_re_modified, rec).groups()    # use the modified RegEx !\n",
    "    for fieldnr, fieldval in enumerate(rec_parsed, 1):\n",
    "        print('{}: {}'.format(fieldnr, fieldval))\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unprocessed records: 1\n"
     ]
    }
   ],
   "source": [
    "content_parsed = []             # the correctly parsed records\n",
    "unprocessed_records = []        # the records not matching the RegEx\n",
    "for rec_id, rec in enumerate(content):\n",
    "    try:\n",
    "        parsed_rec = re.match(apache_re_modified, rec).groups()\n",
    "        content_parsed.append(parsed_rec)\n",
    "    except Exception:\n",
    "        # Error handling: the current records did not match the RegEx, let's store it as uprocessed\n",
    "        unprocessed_records.append((rec_id, rec))\n",
    "print('Number of unprocessed records: {}'.format( len(unprocessed_records)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have still a single unprocessed line, but this one is because of data error. **Note:** the missing ``\"`` (double quote) character at the end of the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8898, '46.118.127.106 - - [20/May/2015:12:05:17 +0000] \"GET /scripts/grok-py-test/configlib.py HTTP/1.1\" 200 235 \"-\" \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(unprocessed_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Format and Fields\n",
    "\n",
    "Except for fields 2 and 3 it's not difficult to guess which field is containing what information.\n",
    "\n",
    "1. IP address\n",
    "2. ?\n",
    "3. ?\n",
    "4. Timestamp\n",
    "5. HTTP verb, URL and HTTP version\n",
    "6. HTTP response code\n",
    "7. Amount of bytes transmitted\n",
    "8. Referring page, i.e.: the page that has referred to the URL in field 6)\n",
    "9. User-Agent string\n",
    "\n",
    "To find out the meaning of fields 2 and 3, let's try to find recrods that have something else than the ``\"-\"`` value. As the following cell shows, there are no log records where fields 2 and 3 have an other than ``\"-\"`` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "records = [ elem for elem in content_parsed if elem[1] != '-' or elem[2] != '-' ]\n",
    "print(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up and convert the data\n",
    "\n",
    "The ``content_parsed`` list contains the parsed log records, but the data is not yet in its most optimal form. Before we can start analyzing, we need to:\n",
    "\n",
    "1. Remove redundant fields, i.e.: fields 2 and 3, which only contain meaningless ``-`` (dash) characters\n",
    "2. convert the fields:\n",
    "   - Timestamp: ``str -> datetime``\n",
    "   - HTTP return code: ``str -> int``\n",
    "   - Amount of bytes transmitted in respons: ``str -> int``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "convert_timestamp = lambda timestamp: datetime.datetime.strptime(timestamp, '%d/%b/%Y:%H:%M:%S %z')\n",
    "def convert_nr_bytes(nr_bytes):\n",
    "    try:\n",
    "        return int(nr_bytes)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def convert_rec(rec):\n",
    "    ipaddr, throwaway1, throwaway2, timestamp, req_url, resp_code, nr_bytes, ref_url, agent_str = rec\n",
    "    return (ipaddr, convert_timestamp(timestamp), req_url, int(resp_code),\n",
    "            convert_nr_bytes(nr_bytes), ref_url, agent_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample of the cleaned up data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('195.14.103.53', datetime.datetime(2015, 5, 19, 11, 5, 15, tzinfo=datetime.timezone.utc), 'GET /reset.css HTTP/1.1', 200, 1015, 'http://www.semicomplete.com/projects/xdotool/', 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/32.0.1700.102 Chrome/32.0.1700.102 Safari/537.36') ------------------------------------------------------------\n",
      "('207.241.237.224', datetime.datetime(2015, 5, 18, 13, 5, 43, tzinfo=datetime.timezone.utc), 'GET /blog/tags/barcampblock HTTP/1.0', 200, 18369, 'http://www.semicomplete.com/blog/tags/year%20review', 'Mozilla/5.0 (compatible; archive.org_bot +http://www.archive.org/details/archive.org_bot)') ------------------------------------------------------------\n",
      "('46.105.14.53', datetime.datetime(2015, 5, 17, 23, 5, 11, tzinfo=datetime.timezone.utc), 'GET /blog/tags/puppet?flav=rss20 HTTP/1.1', 200, 14872, '-', 'UniversalFeedParser/4.2-pre-314-svn +http://feedparser.org/') ------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "content_cleaned = tuple(map(convert_rec, content_parsed))\n",
    "for rec in random_sample(content_cleaned):\n",
    "    print(rec, sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the 10 most frequent IP addresses?\n",
    "\n",
    "What are the most frequent IP addresses in the log? How many times do they occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('66.249.73.135', 482), ('46.105.14.53', 364), ('130.237.218.86', 357), ('75.97.9.59', 273), ('50.16.19.13', 113), ('209.85.238.199', 102), ('68.180.224.225', 99), ('100.43.83.137', 84), ('208.115.111.72', 83), ('198.46.149.143', 82)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "ip_cntr = collections.Counter(getcol(content_cleaned, 0))\n",
    "print(ip_cntr.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many distinct IP address are there?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct IP addresses: 1753\n"
     ]
    }
   ],
   "source": [
    "print('The number of distinct IP addresses:', len(ip_cntr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the timespan of the logs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timespan: 2015-05-17 10:05:00+00:00 - 2015-05-20 21:05:59+00:00 (3 days, 11:00:59)\n"
     ]
    }
   ],
   "source": [
    "def minmax(collection):\n",
    "    '''Return the smallest and largest elements of the collection    \n",
    "    '''\n",
    "    mi, ma = None, None\n",
    "    for rec in collection:\n",
    "        if not(mi): mi = rec     # in 1st round mi = current record\n",
    "        if not(ma): ma = rec     # in 1st round ma = current record\n",
    "        if rec < mi: mi = rec    # update ``mi`` if current record is smaller/less/earlier\n",
    "        if rec > ma: ma = rec    # update ``ma`` if current record is larger/more/later\n",
    "    return mi, ma\n",
    "        \n",
    "earliest, latest = minmax(getcol(content_cleaned, 1))\n",
    "#latest = max(getcol(content_cleaned, 1))\n",
    "print('Timespan: {} - {} ({})'.format(earliest, latest, latest-earliest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Agent statistics\n",
    "\n",
    "- How many different User-Agents?\n",
    "- What are the 10 most frequently occuring User-Agents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- There are 558 distinct agent strings in the logs.\n",
      "- The top 10 most frequent agents are:\n",
      "  - Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.107 Safari/537.36 (1044 times)\n",
      "  - Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.91 Safari/537.36 (369 times)\n",
      "  - UniversalFeedParser/4.2-pre-314-svn +http://feedparser.org/ (364 times)\n",
      "  - Mozilla/5.0 (Windows NT 6.1; WOW64; rv:27.0) Gecko/20100101 Firefox/27.0 (296 times)\n",
      "  - Mozilla/5.0 (iPhone; CPU iPhone OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5376e Safari/8536.25 (compatible; Googlebot/2.1; +http://www.google.com/bot.html) (271 times)\n",
      "  - Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.107 Safari/537.36 (268 times)\n",
      "  - Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html) (237 times)\n",
      "  - Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:27.0) Gecko/20100101 Firefox/27.0 (236 times)\n",
      "  - Mozilla/5.0 (X11; Linux x86_64; rv:27.0) Gecko/20100101 Firefox/27.0 (229 times)\n",
      "  - Tiny Tiny RSS/1.11 (http://tt-rss.org/) (198 times)\n"
     ]
    }
   ],
   "source": [
    "agents = collections.Counter(getcol(content_cleaned,6))\n",
    "print('- There are {} distinct agent strings in the logs.'.format(len(agents)))\n",
    "print('- The top 10 most frequent agents are:')\n",
    "for agent_str, seen in agents.most_common(10):\n",
    "    print('  - {} ({} times)'.format(agent_str, seen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Search engine statistics\n",
    "\n",
    "The search engines are recognizable from their User-Agent string.\n",
    "\n",
    "#### Find out which Google related User-Agent strings are there\n",
    "\n",
    "It would seem that the 'Googlebot' string is what identifies the Google Search Engine. (for more visit http://www.google.com/bot.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (iPhone; CPU iPhone OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5376e Safari/8536.25 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\n",
      "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\n",
      "Feedfetcher-Google; (+http://www.google.com/feedfetcher.html; 3 subscribers; feed-id=14171215010336145331)\n",
      "Feedfetcher-Google; (+http://www.google.com/feedfetcher.html; 1 subscribers; feed-id=11390274670024826467)\n",
      "DoCoMo/2.0 N905i(c100;TB;W24H16) (compatible; Googlebot-Mobile/2.1; +http://www.google.com/bot.html)\n",
      "Feedly/1.0 (+http://www.feedly.com/fetcher.html; like FeedFetcher-Google)\n",
      "Feedfetcher-Google; (+http://www.google.com/feedfetcher.html; 16 subscribers; feed-id=3389821348893992437)\n",
      "Feedfetcher-Google; (+http://www.google.com/feedfetcher.html; 22 subscribers; feed-id=8321906634162087507)\n",
      "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.107 Safari/537.36,gzip(gfe) (via docs.google.com/viewer)\n",
      "Feedfetcher-Google; (+http://www.google.com/feedfetcher.html; 1 subscribers; feed-id=8003088278248648013)\n",
      "SAMSUNG-SGH-E250/1.0 Profile/MIDP-2.0 Configuration/CLDC-1.1 UP.Browser/6.2.3.3.c.1.101 (GUI) MMP/2.0 (compatible; Googlebot-Mobile/2.1; +http://www.google.com/bot.html)\n",
      "Mozilla/5.0 (compatible; Google Desktop/5.9.1005.12335; http://desktop.google.com/)\n",
      "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko; Google Web Preview) Chrome/27.0.1453 Safari/537.36\n",
      "Mozilla/5.0 (Windows; U; Windows NT 5.1; de; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7 (via ggpht.com GoogleImageProxy)\n",
      "Mozilla/5.0 (Windows NT 6.1; rv:6.0) Gecko/20110814 Firefox/6.0 Google favicon\n",
      "Googlebot-Image/1.0\n",
      "Mozilla/5.0 (compatible; Googlebot/2.1;+http://www.google.com/bot.html)\n",
      "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.76 Safari/537.36,gzip(gfe) (via docs.google.com/viewer)\n",
      "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:30.0) Gecko/20100101 Firefox/30.0 AppEngine-Google; (+http://code.google.com/appengine; appid: s~copyliugoa)\n"
     ]
    }
   ],
   "source": [
    "for a in filter(lambda s: 'google' in s.lower(), agents):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Records with the string \"Googlebot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 visits in the logs.\n",
      " - Mozilla/5.0 (iPhone; CPU iPhone OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5376e Safari/8536.25 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\n",
      " - Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\n",
      " - DoCoMo/2.0 N905i(c100;TB;W24H16) (compatible; Googlebot-Mobile/2.1; +http://www.google.com/bot.html)\n",
      " - SAMSUNG-SGH-E250/1.0 Profile/MIDP-2.0 Configuration/CLDC-1.1 UP.Browser/6.2.3.3.c.1.101 (GUI) MMP/2.0 (compatible; Googlebot-Mobile/2.1; +http://www.google.com/bot.html)\n",
      " - Googlebot-Image/1.0\n",
      " - Mozilla/5.0 (compatible; Googlebot/2.1;+http://www.google.com/bot.html)\n"
     ]
    }
   ],
   "source": [
    "rec_googlebot = tuple( filter(lambda s: 'googlebot' in s.lower(), agents))\n",
    "print('There are {} visits in the logs.'.format(len(rec_googlebot)))\n",
    "for rec in rec_googlebot:\n",
    "    print(' -', rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Desktop crawler\n",
    "\n",
    "According to https://support.google.com/webmasters/answer/1061943?hl=en, the User-Agent string of the Google crawler is similar to this RegEx: ``r'^Mozilla.5.0 .+compatible; Googlebot.2.1; ?.http'``. To find out the date of the latest visit:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There have been 238 visits formt he Google crawler.\n",
      "The average interval between visits have been 1255.0 seconds / 20.9 minutes.\n"
     ]
    }
   ],
   "source": [
    "google_crawler_re = r'^Mozilla.5.0 .+compatible; Googlebot.2.1; ?.http'\n",
    "filter_func = lambda rec: re.match(google_crawler_re, rec[6]) and (not 'iphone' in rec[6].lower())\n",
    "\n",
    "google_crawler_records = tuple(filter(filter_func, content_cleaned))\n",
    "google_crawler_visits_nr = len(google_crawler_records)\n",
    "print('There have been {} visits formt he Google crawler.'.format(google_crawler_visits_nr))\n",
    "first_visit, last_visit = minmax(getcol(google_crawler_records, 1))\n",
    "d = (last_visit - first_visit) / len(google_crawler_records)\n",
    "print('The average interval between visits have been {:.1f} seconds / {:.1f} minutes.'.format(d.seconds, d.seconds/60))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3.6 (system_wide)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
